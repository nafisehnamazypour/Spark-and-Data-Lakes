# The purpose of this database
    Sparkify which is a music startup, has a database based on their users and songs. 
    Their goal is to transfer their processes and data to the cloud. Their data is stored in S3,
    in a directory with JSON files on the songs in their app and a directory with JSON files on user activity.
    The Data Engineer should develope an ELT pipeline that pulls data from S3,
    process the data into analytics tables using Spark, and load them back into S3. 
    In this project the Spark process is deployed on a cluster using AWS.


# Database schema design:
    The whole database should have a star schema.     
    It has a Fact table named "songplays" and four dimension tables as follows:
    users, songs, artists and time. The parameters of these relations are as follows:
    
    songplay table parameters:  = songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

    user table parameters: user_id, first_name, last_name, gender, level 

    song table parameters: 'song_id', 'title', 'artist_id', 'year', 'duration'

    artist table parameters: artist_id, name, location, lattitude, longitude

    time table parameters: start_time, hour, day, week, month, year, weekday


    The test data for this project is on our workspace. But, in the real program run the data is in S3 bucket. Configurations can be seen in the dl.cfg file.
            Song data: s3://udacity-dend/song_data
            Log data: s3://udacity-dend/log_data
    The song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 
    The log dataset includes log files in JSON format generated by the event simulator based on the songs in the dataset.
    
    etl.py includes ELT pipeline which reads data from S3, processes that data using Spark, and writes them back to S3.  
    In the processing data, the star schema mentioned above is created using spark, some user-defined functions and spark data frames. 
    The columns are extracted and then table columns are filled. 
